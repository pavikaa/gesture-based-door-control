{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "uvDA8AK7QOq-",
        "Gxk414PU3oy3",
        "d5_61831d5AM",
        "a9g2n41p24nR",
        "kxA0zCOaS35v",
        "DG3m-VpE1zOd",
        "CRjvkFQy2RgS",
        "j7DO6xxXVCym",
        "ykccQn7SXrUX"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvDA8AK7QOq-"
      },
      "source": [
        "## Setup Python Environment \n",
        "\n",
        "To set up the Python environment and install the necessary dependencies for the notebook, please run the following cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2gs-PL4xDkZ"
      },
      "source": [
        "# Remove all files and directories in the current working directory\n",
        "!rm -r *\n",
        "\n",
        "# Install the 'xxd' package\n",
        "!apt-get -qq install xxd\n",
        "\n",
        "# Install required Python packages for data manipulation, numerical computations, and data visualization\n",
        "!pip install pandas numpy matplotlib\n",
        "\n",
        "# Install TensorFlow\n",
        "!pip install tensorflow==2.12.0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lwkeshJk7dg"
      },
      "source": [
        "## Upload Data\n",
        "\n",
        "1. Click on the \"Files\" tab located in the left-side menu of Colab.\n",
        "2. Drag and drop your desired `.csv` files from your computer onto the \"Files\" tab. <br>\n",
        "Alternatively, you can click on the \"Upload\" button within the \"Files\" tab and browse for the files you want to upload.\n",
        "By following these steps, you will successfully upload your CSV files into Colab for further processing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gxk414PU3oy3"
      },
      "source": [
        "## Parse and prepare the data\n",
        "\n",
        "To parse and prepare the data in the next cell, you need to update the `GESTURES` list with the gesture data you've collected in `.csv` format.\n",
        "\n",
        "1.  Replace the values in the `GESTURES` list with the names of the gestures you have collected. For example:\n",
        "\n",
        "`GESTURES = [\n",
        "    \"gesture_1\",\n",
        "    \"gesture_2\",\n",
        "    \"gesture_3\"\n",
        "]`\n",
        "\n",
        "2.  Make sure the names in the `GESTURES` list match the actual filenames of your CSV files.\n",
        "\n",
        "By updating the `GESTURES` list, the code will correctly process and transform the CSV files of the corresponding gestures for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGChd1FAk5_j"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "SEED = 1111\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "# Define the list of gestures\n",
        "GESTURES = [\n",
        "    \"c_up_push\",\n",
        "    \"other\"\n",
        "]\n",
        "\n",
        "# Define the number of samples per gesture\n",
        "SAMPLES_PER_GESTURE = 100\n",
        "\n",
        "# Get the number of gestures\n",
        "NUM_GESTURES = len(GESTURES)\n",
        "\n",
        "# Create one-hot encoded representations of the gestures\n",
        "ONE_HOT_ENCODED_GESTURES = np.eye(NUM_GESTURES)\n",
        "\n",
        "# Initialize lists to store inputs and outputs\n",
        "inputs = []\n",
        "outputs = []\n",
        "\n",
        "# Iterate over each gesture\n",
        "for gesture_index in range(NUM_GESTURES):\n",
        "    gesture = GESTURES[gesture_index]\n",
        "    print(f\"Processing index {gesture_index} for gesture '{gesture}'.\")\n",
        "\n",
        "    # Get the one-hot encoded output for the current gesture\n",
        "    output = ONE_HOT_ENCODED_GESTURES[gesture_index]\n",
        "\n",
        "    # Read the data from the CSV file for the current gesture\n",
        "    df = pd.read_csv(\"/content/\" + gesture + \".csv\")\n",
        "\n",
        "    # Calculate the number of recordings for the current gesture\n",
        "    num_recordings = int(df.shape[0] / SAMPLES_PER_GESTURE)\n",
        "\n",
        "    print(f\"\\tThere are {num_recordings} recordings of the {gesture} gesture.\")\n",
        "\n",
        "    # Iterate over each recording\n",
        "    for i in range(num_recordings):\n",
        "        tensor = []\n",
        "\n",
        "        # Iterate over each sample in the recording\n",
        "        for j in range(SAMPLES_PER_GESTURE):\n",
        "            index = i * SAMPLES_PER_GESTURE + j\n",
        "\n",
        "            # Normalize and scale the accelerometer and gyroscope data\n",
        "            tensor += [\n",
        "                (df['accelerometerX'][index] + 4) / 8,\n",
        "                (df['accelerometerY'][index] + 4) / 8,\n",
        "                (df['accelerometerZ'][index] + 4) / 8,\n",
        "                (df['gyroscopeX'][index] + 2000) / 4000,\n",
        "                (df['gyroscopeY'][index] + 2000) / 4000,\n",
        "                (df['gyroscopeZ'][index] + 2000) / 4000\n",
        "            ]\n",
        "\n",
        "        # Append the tensor as input and the output to the respective lists\n",
        "        inputs.append(tensor)\n",
        "        outputs.append(output)\n",
        "\n",
        "# Convert the input and output lists to numpy arrays\n",
        "inputs = np.array(inputs)\n",
        "outputs = np.array(outputs)\n",
        "\n",
        "print(\"Data set parsing and preparation complete.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5_61831d5AM"
      },
      "source": [
        "## Randomize and split the input and output pairs for training\n",
        "\n",
        "In order to train the model effectively, the input and output pairs need to be randomized and split into different sets for training, validation, and testing. Here's how the data is divided:\n",
        "\n",
        "- Training Set: This set comprises 60% of the total input and output pairs. It is used to train the model by optimizing its parameters and updating the weights based on the provided input-output pairs.\n",
        "\n",
        "- Validation Set: This set consists of 20% of the total input and output pairs. It is used to measure the model's performance during training. The validation set helps assess how well the model generalizes to unseen data and allows for monitoring the model's progress.\n",
        "\n",
        "- Testing Set: This set also represents 20% of the total input and output pairs. It is used to test the model's performance after the training phase is completed. The testing set evaluates the model's ability to make accurate predictions on new, unseen data.\n",
        "\n",
        "By splitting the data into these sets, you can train, validate, and evaluate the model's performance throughout the training process, ensuring reliable and accurate results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfNEmUZMeIEx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edba1599-dd14-441a-de4f-889d28866d5b"
      },
      "source": [
        "# Get the total number of inputs\n",
        "num_inputs = len(inputs)\n",
        "\n",
        "# Create an array of indices from 0 to num_inputs\n",
        "randomize = np.arange(num_inputs)\n",
        "\n",
        "# Shuffle the indices randomly\n",
        "np.random.shuffle(randomize)\n",
        "\n",
        "# Randomize the order of inputs and outputs using the randomized indices\n",
        "inputs = inputs[randomize]\n",
        "outputs = outputs[randomize]\n",
        "\n",
        "# Split the data into three sets: training, testing, and validation\n",
        "TRAIN_SPLIT = int(0.6 * num_inputs)\n",
        "TEST_SPLIT = int(0.2 * num_inputs + TRAIN_SPLIT)\n",
        "\n",
        "# Split the inputs based on the defined splits\n",
        "inputs_train, inputs_test, inputs_validate = np.split(inputs, [TRAIN_SPLIT, TEST_SPLIT])\n",
        "\n",
        "# Split the outputs based on the defined splits\n",
        "outputs_train, outputs_test, outputs_validate = np.split(outputs, [TRAIN_SPLIT, TEST_SPLIT])\n",
        "\n",
        "print(\"Data set randomization and splitting complete.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data set randomization and splitting complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9g2n41p24nR"
      },
      "source": [
        "## Build & Train the Model\n",
        "\n",
        "Build and train a [TensorFlow](https://www.tensorflow.org) model using the high-level [Keras](https://www.tensorflow.org/guide/keras) API."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGNFa-lX24Qo"
      },
      "source": [
        "# Create a sequential model\n",
        "model = tf.keras.Sequential()\n",
        "\n",
        "# Add a dense layer with 50 units and ReLU activation function\n",
        "model.add(tf.keras.layers.Dense(50, activation='relu'))\n",
        "\n",
        "# Add a dense layer with 15 units and ReLU activation function\n",
        "model.add(tf.keras.layers.Dense(15, activation='relu'))\n",
        "\n",
        "# Add a dense layer with NUM_GESTURES units and softmax activation function\n",
        "# Softmax is used because we expect only one gesture to occur per input\n",
        "model.add(tf.keras.layers.Dense(NUM_GESTURES, activation='softmax'))\n",
        "\n",
        "# Compile the model with optimizer, loss, and metrics\n",
        "model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
        "\n",
        "# Train the model\n",
        "# Use inputs_train and outputs_train as training data\n",
        "# Train for 600 epochs with a batch size of 1\n",
        "# Use inputs_validate and outputs_validate as validation data\n",
        "history = model.fit(inputs_train, outputs_train, epochs=600, batch_size=1, validation_data=(inputs_validate, outputs_validate))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxA0zCOaS35v"
      },
      "source": [
        "## Graph the loss\n",
        "\n",
        "Graph the loss to see when the model stops improving."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvFNHXoQzmcM"
      },
      "source": [
        "# Increase the size of the graphs to (20, 10)\n",
        "plt.rcParams[\"figure.figsize\"] = (20, 10)\n",
        "\n",
        "# Retrieve the loss values from the training history\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "# Create an array of epochs from 1 to the length of the loss values\n",
        "epochs = range(1, len(loss) + 1)\n",
        "\n",
        "# Plot the training loss as green dots\n",
        "plt.plot(epochs, loss, 'g.', label='Training loss')\n",
        "\n",
        "# Plot the validation loss as a solid blue line\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "\n",
        "# Set the title and labels for the graph\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "\n",
        "# Add a legend to the graph\n",
        "plt.legend()\n",
        "\n",
        "# Display the graph\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DG3m-VpE1zOd"
      },
      "source": [
        "## Graph the loss again, skipping a bit of the start\n",
        "\n",
        "We'll graph the same data as the previous code cell, but start at index 100 so we can further zoom in once the model starts to converge."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3xT7ue2zovd"
      },
      "source": [
        "# Define the number of epochs to skip at the start\n",
        "SKIP = 100\n",
        "\n",
        "# Plot the training loss starting from SKIP epoch\n",
        "plt.plot(epochs[SKIP:], loss[SKIP:], 'g.', label='Training loss')\n",
        "\n",
        "# Plot the validation loss starting from SKIP epoch\n",
        "plt.plot(epochs[SKIP:], val_loss[SKIP:], 'b.', label='Validation loss')\n",
        "\n",
        "# Set the title and labels for the graph\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "\n",
        "# Add a legend to the graph\n",
        "plt.legend()\n",
        "\n",
        "# Display the graph\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRjvkFQy2RgS"
      },
      "source": [
        "## Graph the mean absolute error\n",
        "\n",
        "[Mean absolute error](https://en.wikipedia.org/wiki/Mean_absolute_error) is another metric to judge the performance of the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBjCf1-2zx9C"
      },
      "source": [
        "# Retrieve the mean absolute error (MAE) values from the training history\n",
        "mae = history.history['mae']\n",
        "val_mae = history.history['val_mae']\n",
        "\n",
        "# Plot the training MAE starting from SKIP epoch\n",
        "plt.plot(epochs[SKIP:], mae[SKIP:], 'g.', label='Training MAE')\n",
        "\n",
        "# Plot the validation MAE starting from SKIP epoch\n",
        "plt.plot(epochs[SKIP:], val_mae[SKIP:], 'b.', label='Validation MAE')\n",
        "\n",
        "# Set the title and labels for the graph\n",
        "plt.title('Training and validation mean absolute error')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('MAE')\n",
        "\n",
        "# Add a legend to the graph\n",
        "plt.legend()\n",
        "\n",
        "# Display the graph\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7DO6xxXVCym"
      },
      "source": [
        "## Convert the Trained Model to Tensor Flow Lite\n",
        "\n",
        "In the next cell, the model is converted to the TensorFlow Lite format, and the size of the model in bytes is printed out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Xn1-Rn9Cp_8"
      },
      "source": [
        "# Convert the model to TensorFlow Lite format without quantization\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the converted model to disk\n",
        "open(\"gesture_model.tflite\", \"wb\").write(tflite_model)\n",
        "\n",
        "# Get the size of the saved model file\n",
        "import os\n",
        "basic_model_size = os.path.getsize(\"gesture_model.tflite\")\n",
        "\n",
        "# Print the size of the saved model\n",
        "print(\"Model is %d bytes\" % basic_model_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykccQn7SXrUX"
      },
      "source": [
        "## Encode the Model in an Arduino Header File \n",
        "\n",
        "The next cell creates a constant byte array that contains the TensorFlow Lite model. The provided code converts the model to an Arduino header file format, which can be easily included and used in your Arduino sketch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9J33uwpNtAku"
      },
      "source": [
        "# Create the model.h file and write the initial line\n",
        "!echo \"const unsigned char model[] = {\" > /content/model.h\n",
        "\n",
        "# Convert the content of gesture_model.tflite to hexadecimal representation and append it to model.h\n",
        "!cat gesture_model.tflite | xxd -i >> /content/model.h\n",
        "\n",
        "# Write the closing line to model.h\n",
        "!echo \"};\" >> /content/model.h\n",
        "\n",
        "# Get the size of the model.h file\n",
        "import os\n",
        "model_h_size = os.path.getsize(\"model.h\")\n",
        "\n",
        "# Print the size of the model.h file\n",
        "print(f\"Header file, model.h, is {model_h_size:,} bytes.\")\n",
        "print(\"\\nOpen the side panel (refresh if needed). Double click model.h to download the file.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eSkHZaLzMId"
      },
      "source": [
        "# Classifying IMU Data\n",
        "\n",
        "Now it's time to switch back to `imu_classification` and run our new model on the Arduino Nano 33 BLE Sense to classify the accelerometer and gyroscope data.\n"
      ]
    }
  ]
}